{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ Generating High-Credibility Synthetic Healthcare Claims\n",
    "\n",
    "This notebook uses instruction-tuned text generation to create synthetic healthcare claims and explanations using LaMini-Flan-T5-248M. These synthetic samples supplement real-world data, helping correct label imbalance and improve classifier robustness by introducing more credible (true) examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… 1. Imports & Model Pipeline Setup\n",
    "\n",
    "We import required libraries and load the LaMini-Flan-T5-248M model via Hugging Face's `pipeline`. This lightweight encoder-decoder transformer is instruction-tuned for general reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers --quiet\n",
    "\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load LaMini-Flan reasoning model\n",
    "model_name = \"MBZUAI/LaMini-Flan-T5-248M\"\n",
    "reasoning_pipeline = pipeline(\"text2text-generation\", model=model_name, device=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ€ 2. Generate Synthetic Claims in Batch\n",
    "\n",
    "We generate 100+ responses (10 claims per topic) from the model using the defined prompt. Each output is a plausible, fact-like healthcare claim, although not guaranteed to be medically verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 100 prompts for the model to expand on\n",
    "base_prompts = [\n",
    "    \"The claim is that exercise helps prevent heart disease.\",\n",
    "    \"The claim is that vaccines are effective at preventing illness.\",\n",
    "    \"The claim is that meditation reduces anxiety.\",\n",
    "    \"The claim is that regular sleep improves brain function.\",\n",
    "    \"The claim is that sunscreen prevents skin cancer.\",\n",
    "    \"The claim is that fiber helps with digestion.\",\n",
    "    \"The claim is that drinking water supports kidney health.\",\n",
    "    \"The claim is that low sodium intake benefits blood pressure.\",\n",
    "    \"The claim is that probiotics support gut health.\",\n",
    "    \"The claim is that dental hygiene impacts heart health.\",\n",
    "    # Repeat and randomly vary structure\n",
    "]\n",
    "# Pad out to 100 with variations\n",
    "while len(base_prompts) < 100:\n",
    "    health_topic = random.choice([\n",
    "        \"exercise\", \"hydration\", \"mental health\", \"nutrition\",\n",
    "        \"disease prevention\", \"vaccination\", \"chronic illness\", \"cancer prevention\"\n",
    "    ])\n",
    "    action = random.choice([\n",
    "        \"helps with\", \"is important for\", \"is linked to\", \"is known to reduce\", \"supports\"\n",
    "    ])\n",
    "    outcome = random.choice([\n",
    "        \"heart health\", \"reduced stress\", \"stronger immunity\", \"lower cancer risk\",\n",
    "        \"lower blood pressure\", \"improved sleep\", \"gut health\"\n",
    "    ])\n",
    "    prompt = f\"The claim is that {health_topic} {action} {outcome}.\"\n",
    "    base_prompts.append(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing and importing the required packages and libraries, we load the model and we generated a list of 100 prompts for the reasoning model to expand upon.\n",
    "\n",
    "1. **Expanding the List to 100 Prompts**:\n",
    "   - The `while` loop continues to generate new prompts until the total count reaches 100. For each new prompt, random health topics, actions, and outcomes are chosen from predefined lists to vary the structure of the claims.\n",
    "\n",
    "2. **Result**:\n",
    "   - The final list `base_prompts` contains 100 unique prompts that can be used to generate explanations and assess the reasoning model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¬ 3. Generate Natural Language Explanations\n",
    "We pass each claim + score into LaMini-Flan using the prompt:\n",
    "\n",
    "```\n",
    "Claim: {claim}\n",
    "Credibility: {score}%\n",
    "Explain why this claim is likely accurate.\n",
    "```\n",
    "\n",
    "Each explanation aims to justify the claim using scientific-sounding reasoning based on the model's pretrained knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Generating explanations for synthetic claims...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [04:12<00:00,  2.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate explanations\n",
    "claims = []\n",
    "explanations = []\n",
    "\n",
    "print(\"ðŸ§  Generating explanations for synthetic claims...\")\n",
    "for claim_text in tqdm(base_prompts):\n",
    "    # Strip down to clean claim for CSV\n",
    "    clean_claim = claim_text.replace(\"The claim is that \", \"\").strip().rstrip(\".\")\n",
    "\n",
    "    # Make the prompt explicit for explanation\n",
    "    prompt = f\"Claim: {clean_claim}\\nCredibility: 90%\\nExplain why this claim is likely accurate.\"\n",
    "\n",
    "    output = reasoning_pipeline(prompt, max_length=150, num_return_sequences=1)[0][\"generated_text\"]\n",
    "\n",
    "    claims.append(clean_claim)\n",
    "    explanations.append(output.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to generate explanations for a set of synthetic claims using the reasoning model.\n",
    "\n",
    "1. **Generating Explanations**:\n",
    "   - A loop iterates through each claim in `base_prompts`, cleaning the claim by removing the prefix \"The claim is that\" and stripping any trailing punctuation.\n",
    "\n",
    "2. **Creating Prompts**:\n",
    "   - For each cleaned claim, a prompt is created with a fixed credibility of 90% and the instruction to explain why the claim is likely accurate.\n",
    "\n",
    "3. **Model Inference**:\n",
    "   - The `reasoning_pipeline` is used to generate an explanation for each claim. The explanation is returned as the model's output and is appended to the `explanations` list.\n",
    "\n",
    "4. **Storing Results**:\n",
    "   - Both the cleaned claims and their corresponding explanations are stored in separate lists, `claims` and `explanations`, which can be used for further evaluation or saving to a file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§® 4. Assign High Credibility Scores (Simulated)\n",
    "\n",
    "\n",
    "Since all claims are intentionally designed to be accurate, we simulate a score range between 71â€“95%, mimicking the distribution of credible examples used during classifier training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign credibility scores and clean prompts\n",
    "clean_claims = [c.replace(\"The claim is that \", \"\").strip().rstrip(\".\") for c in claims]\n",
    "scores = [random.randint(71, 95) for _ in range(len(clean_claims))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we clean the claim texts by removing the prefix \"The claim is that\" and trimming any trailing punctuation.The cleaned claims and their associated scores are stored in the `clean_claims` and `scores` lists, respectively. These scores simulate varying levels of credibility for each claim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š 5. Final Dataset Preview\n",
    "\n",
    "We preview the final set of synthetic samples, each consisting of:\n",
    "\n",
    "- A claim\n",
    "- A simulated credibility score\n",
    "- A generated explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"claim\": clean_claims,\n",
    "    \"score\": scores,\n",
    "    \"explanation\": explanations\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ 6. Save Synthetic Dataset to CSV\n",
    "\n",
    "We save the cleaned, formatted dataset as synthetic_claim_explanations.csv, which is later used in the Streamlit app for:\n",
    "\n",
    "- Demonstration\n",
    "- Bias mitigation\n",
    "- Evaluation consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 100+ synthetic claims to 'synthetic_claim_explanations.csv'\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"synthetic_claim_explanations.csv\", index=False)\n",
    "print(\"âœ… Saved 100+ synthetic claims to 'synthetic_claim_explanations.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
