{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# ✅ 1. Install and Import\n",
    "# ---------------------------------------------\n",
    "!pip install transformers --quiet\n",
    "\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load reasoning model\n",
    "model_name = \"MBZUAI/LaMini-Flan-T5-783M\"\n",
    "lamini_pipe = pipeline(\"text2text-generation\", model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We install and import the required libraries as our first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# ✅ 2. Define Topics for Claim Generation\n",
    "# ---------------------------------------------\n",
    "topics = [\n",
    "    \"vaccination\", \"heart disease\", \"hydration\", \"mental health\", \"gut health\",\n",
    "    \"diabetes\", \"nutrition\", \"antibiotics\", \"fertility\", \"cancer prevention\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we defined a list of topics that will be used to generate healthcare claims.\n",
    "\n",
    "1. **Topic List**:\n",
    "   - The `topics` list contains common healthcare topics, such as `\"vaccination\"`, `\"heart disease\"`, `\"hydration\"`, and others. These topics are intended to serve as the basis for generating healthcare-related claims.\n",
    "\n",
    "2. **Purpose**:\n",
    "   - These topics will be used in further steps to generate claims for testing or training the reasoning model. Each claim will be related to one of these topics, and the model will generate explanations based on the input claim and its credibility score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:30<00:00,  9.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 total claims.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# ✅ 3. Generate Claims for Each Topic\n",
    "# ---------------------------------------------\n",
    "def generate_claims(topic):\n",
    "    prompt = f\"Generate 3 healthcare claims about {topic}, some true and some false.\"\n",
    "    output = lamini_pipe(prompt, max_length=128, do_sample=True, temperature=0.9)[0][\"generated_text\"]\n",
    "    claims = [c.strip(\"•- \") for c in output.split(\"\\n\") if c.strip()]\n",
    "    return claims\n",
    "\n",
    "all_claims = []\n",
    "for topic in tqdm(topics):\n",
    "    all_claims.extend(generate_claims(topic))\n",
    "\n",
    "print(f\"Generated {len(all_claims)} total claims.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the list, we generated healthcare claims for each topic by prompting the reasoning model to produce a mix of true and false claims. The `generate_claims` function generates three claims per topic, which are cleaned and stored in a list. The code loops through a predefined set of topics, collecting all generated claims in one list and printing the total count of claims created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:18<00:00,  7.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# ✅ 4. Assign Credibility + Generate Explanations\n",
    "# ---------------------------------------------\n",
    "def assign_score():\n",
    "    # Evenly sample across bins: 10%, 30%, 50%, 70%, 90%\n",
    "    return random.choice([10, 30, 50, 70, 90])\n",
    "\n",
    "synthetic_data = []\n",
    "\n",
    "for claim in tqdm(all_claims):\n",
    "    score = assign_score()\n",
    "    reasoning_type = \"accurate\" if score > 70 else \"misinformation\"\n",
    "    prompt = f\"Claim: {claim}\\nCredibility: {score}%\\nExplain why this claim is likely {reasoning_type}.\"\n",
    "    explanation = lamini_pipe(prompt, max_length=100, do_sample=False)[0][\"generated_text\"]\n",
    "\n",
    "    synthetic_data.append({\n",
    "        \"claim\": claim,\n",
    "        \"credibility\": score,\n",
    "        \"explanation\": explanation.strip()\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is assigning a synthetic credibility score to each claim and generates an explanation based on the score.\n",
    "\n",
    "1. **Assigning Credibility**:\n",
    "   - The `assign_score` function randomly selects a credibility score from predefined values: 10%, 30%, 50%, 70%, or 90%. This simulates different levels of credibility for the claims.\n",
    "\n",
    "2. **Generating Explanations**:\n",
    "   - For each claim, a `reasoning_type` is determined based on the score (accurate if the score is above 70%, misinformation otherwise). A prompt is created for the reasoning model to generate an explanation for why the claim is considered accurate or misinformation.\n",
    "\n",
    "3. **Collecting Synthetic Data**:\n",
    "   - The generated claims, their assigned credibility scores, and the corresponding explanations are stored in a list called `synthetic_data`.\n",
    "\n",
    "This process allows for the creation of a dataset with claims, credibility scores, and explanations for training or evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved synthetic_claim_explanations.csv with shape: (10, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>credibility</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True: Vaccination has been proven to save mill...</td>\n",
       "      <td>50</td>\n",
       "      <td>This claim is likely misinformation because va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1. True: Heart disease is a common health issu...</td>\n",
       "      <td>50</td>\n",
       "      <td>This claim is likely misinformation because he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1. True: Drinking enough water can improve ove...</td>\n",
       "      <td>90</td>\n",
       "      <td>The claim is likely accurate because it is a w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1. Mental health is a complex issue that can b...</td>\n",
       "      <td>30</td>\n",
       "      <td>This claim is likely misinformation because it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True: There is evidence to suggest that gut he...</td>\n",
       "      <td>30</td>\n",
       "      <td>This claim is likely misinformation because it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  credibility  \\\n",
       "0  True: Vaccination has been proven to save mill...           50   \n",
       "1  1. True: Heart disease is a common health issu...           50   \n",
       "2  1. True: Drinking enough water can improve ove...           90   \n",
       "3  1. Mental health is a complex issue that can b...           30   \n",
       "4  True: There is evidence to suggest that gut he...           30   \n",
       "\n",
       "                                         explanation  \n",
       "0  This claim is likely misinformation because va...  \n",
       "1  This claim is likely misinformation because he...  \n",
       "2  The claim is likely accurate because it is a w...  \n",
       "3  This claim is likely misinformation because it...  \n",
       "4  This claim is likely misinformation because it...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# ✅ 5. Save Synthetic Data to CSV\n",
    "# ---------------------------------------------\n",
    "df = pd.DataFrame(synthetic_data)\n",
    "df.to_csv(\"synthetic_claim_explanations.csv\", index=False)\n",
    "\n",
    "print(\"Saved synthetic_claim_explanations.csv with shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📘 1. Install and Import Libraries\n",
    "\n",
    "!pip install transformers tqdm --quiet\n",
    "\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# 📘 2. Load Reasoning Model\n",
    "# Load LaMini-Flan reasoning model\n",
    "model_name = \"MBZUAI/LaMini-Flan-T5-248M\"\n",
    "reasoning_pipeline = pipeline(\"text2text-generation\", model=model_name, device=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📘 3. Generate 100 Prompts\n",
    "# Create 100 prompts for the model to expand on\n",
    "base_prompts = [\n",
    "    \"The claim is that exercise helps prevent heart disease.\",\n",
    "    \"The claim is that vaccines are effective at preventing illness.\",\n",
    "    \"The claim is that meditation reduces anxiety.\",\n",
    "    \"The claim is that regular sleep improves brain function.\",\n",
    "    \"The claim is that sunscreen prevents skin cancer.\",\n",
    "    \"The claim is that fiber helps with digestion.\",\n",
    "    \"The claim is that drinking water supports kidney health.\",\n",
    "    \"The claim is that low sodium intake benefits blood pressure.\",\n",
    "    \"The claim is that probiotics support gut health.\",\n",
    "    \"The claim is that dental hygiene impacts heart health.\",\n",
    "    # Repeat and randomly vary structure\n",
    "]\n",
    "# Pad out to 100 with variations\n",
    "while len(base_prompts) < 100:\n",
    "    health_topic = random.choice([\n",
    "        \"exercise\", \"hydration\", \"mental health\", \"nutrition\",\n",
    "        \"disease prevention\", \"vaccination\", \"chronic illness\", \"cancer prevention\"\n",
    "    ])\n",
    "    action = random.choice([\n",
    "        \"helps with\", \"is important for\", \"is linked to\", \"is known to reduce\", \"supports\"\n",
    "    ])\n",
    "    outcome = random.choice([\n",
    "        \"heart health\", \"reduced stress\", \"stronger immunity\", \"lower cancer risk\",\n",
    "        \"lower blood pressure\", \"improved sleep\", \"gut health\"\n",
    "    ])\n",
    "    prompt = f\"The claim is that {health_topic} {action} {outcome}.\"\n",
    "    base_prompts.append(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing and importing the required packages and libraries, we load the model and we generated a list of 100 prompts for the reasoning model to expand upon.\n",
    "\n",
    "1. **Initial Set of Base Prompts**:\n",
    "   - A list of 10 predefined health-related claims is created, each representing a claim about the effects of various health practices (e.g., exercise, vaccines, meditation, etc.).\n",
    "\n",
    "2. **Expanding the List to 100 Prompts**:\n",
    "   - The `while` loop continues to generate new prompts until the total count reaches 100. For each new prompt, random health topics, actions, and outcomes are chosen from predefined lists to vary the structure of the claims.\n",
    "\n",
    "3. **Result**:\n",
    "   - The final list `base_prompts` contains 100 unique prompts that can be used to generate explanations and assess the reasoning model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Generating explanations for synthetic claims...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:12<00:00,  2.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# 📘 4. Generate Explanations\n",
    "# Generate explanations\n",
    "claims = []\n",
    "explanations = []\n",
    "\n",
    "print(\"🧠 Generating explanations for synthetic claims...\")\n",
    "for claim_text in tqdm(base_prompts):\n",
    "    # Strip down to clean claim for CSV\n",
    "    clean_claim = claim_text.replace(\"The claim is that \", \"\").strip().rstrip(\".\")\n",
    "\n",
    "    # Make the prompt explicit for explanation\n",
    "    prompt = f\"Claim: {clean_claim}\\nCredibility: 90%\\nExplain why this claim is likely accurate.\"\n",
    "\n",
    "    output = reasoning_pipeline(prompt, max_length=150, num_return_sequences=1)[0][\"generated_text\"]\n",
    "\n",
    "    claims.append(clean_claim)\n",
    "    explanations.append(output.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to generate explanations for a set of synthetic claims using the reasoning model.\n",
    "\n",
    "1. **Generating Explanations**:\n",
    "   - A loop iterates through each claim in `base_prompts`, cleaning the claim by removing the prefix \"The claim is that\" and stripping any trailing punctuation.\n",
    "\n",
    "2. **Creating Prompts**:\n",
    "   - For each cleaned claim, a prompt is created with a fixed credibility of 90% and the instruction to explain why the claim is likely accurate.\n",
    "\n",
    "3. **Model Inference**:\n",
    "   - The `reasoning_pipeline` is used to generate an explanation for each claim. The explanation is returned as the model's output and is appended to the `explanations` list.\n",
    "\n",
    "4. **Storing Results**:\n",
    "   - Both the cleaned claims and their corresponding explanations are stored in separate lists, `claims` and `explanations`, which can be used for further evaluation or saving to a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📘 5. Assign Scores and Clean Claim Text\n",
    "# Assign credibility scores and clean prompts\n",
    "clean_claims = [c.replace(\"The claim is that \", \"\").strip().rstrip(\".\") for c in claims]\n",
    "scores = [random.randint(71, 95) for _ in range(len(clean_claims))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we clean the claim texts by removing the prefix \"The claim is that\" and trimming any trailing punctuation. It also assigns a random credibility score between 71% and 95% to each claim. The cleaned claims and their associated scores are stored in the `clean_claims` and `scores` lists, respectively. These scores simulate varying levels of credibility for each claim.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📘 6. Combine into DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"claim\": clean_claims,\n",
    "    \"score\": scores,\n",
    "    \"explanation\": explanations\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined the cleaned claims, assigned scores, and generated explanations into a single DataFrame for easy access and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 100+ synthetic claims to 'synthetic_claim_explanations.csv'\n"
     ]
    }
   ],
   "source": [
    "# 📘 7. Save to CSV\n",
    "df.to_csv(\"synthetic_claim_explanations.csv\", index=False)\n",
    "print(\"✅ Saved 100+ synthetic claims to 'synthetic_claim_explanations.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
